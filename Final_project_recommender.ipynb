{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_text=[0]*231780\n",
    "ratings=[0]*231780\n",
    "itemID=[0]*231780\n",
    "reviewerID=[0]*231780\n",
    "\n",
    "i=0\n",
    "with open(\"reviews_Video_Games_5.json\",\"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        data=json.loads(line) \n",
    "        review_text[i]=data[\"reviewText\"]\n",
    "        reviewerID[i]=data[\"reviewerID\"]\n",
    "        itemID[i]=data[\"asin\"]\n",
    "        ratings[i]=data[\"overall\"]\n",
    "        i+=1\n",
    "\n",
    "reviewerID = pd.DataFrame({'reviewerID': reviewerID})\n",
    "ratings = pd.DataFrame({'Ratings': ratings})\n",
    "itemID = pd.DataFrame({'ItemID':itemID})\n",
    "review_text = pd.DataFrame({'reviewText':review_text})\n",
    "\n",
    "df_total = pd.concat([ratings,reviewerID,itemID,review_text], axis=1)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### review texts group by item rated 5 stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items = df_total[['ItemID','reviewText','reviewerID','Ratings']]\n",
    "items = items[items['Ratings'] == 5]\n",
    "grouped_item = items.groupby('ItemID')\n",
    "items = grouped_item['reviewText'].apply(\"||\".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# item_reviews = df_total[['ItemID','reviewText','Ratings']]\n",
    "# item_reviews = item_reviews[item_reviews['Ratings'] == 5]\n",
    "# grouped_item = item_reviews.groupby('ItemID')\n",
    "# item_review=grouped_item['reviewText'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### review texts group by user rated 5 stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_reviews = df_total[['reviewerID','reviewText','Ratings']]\n",
    "user_reviews = user_reviews[user_reviews['Ratings'] == 5]\n",
    "grouped_user = user_reviews.groupby('reviewerID')\n",
    "user_review=grouped_user['reviewText'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec on item reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10255\n",
      "100 10255\n",
      "200 10255\n",
      "300 10255\n",
      "400 10255\n",
      "500 10255\n",
      "600 10255\n",
      "700 10255\n",
      "800 10255\n",
      "900 10255\n",
      "1000 10255\n",
      "1100 10255\n",
      "1200 10255\n",
      "1300 10255\n",
      "1400 10255\n",
      "1500 10255\n",
      "1600 10255\n",
      "1700 10255\n",
      "1800 10255\n",
      "1900 10255\n",
      "2000 10255\n",
      "2100 10255\n",
      "2200 10255\n",
      "2300 10255\n",
      "2400 10255\n",
      "2500 10255\n",
      "2600 10255\n",
      "2700 10255\n",
      "2800 10255\n",
      "2900 10255\n",
      "3000 10255\n",
      "3100 10255\n",
      "3200 10255\n",
      "3300 10255\n",
      "3400 10255\n",
      "3500 10255\n",
      "3600 10255\n",
      "3700 10255\n",
      "3800 10255\n",
      "3900 10255\n",
      "4000 10255\n",
      "4100 10255\n",
      "4200 10255\n",
      "4300 10255\n",
      "4400 10255\n",
      "4500 10255\n",
      "4600 10255\n",
      "4700 10255\n",
      "4800 10255\n",
      "4900 10255\n",
      "5000 10255\n",
      "5100 10255\n",
      "5200 10255\n",
      "5300 10255\n",
      "5400 10255\n",
      "5500 10255\n",
      "5600 10255\n",
      "5700 10255\n",
      "5800 10255\n",
      "5900 10255\n",
      "6000 10255\n",
      "6100 10255\n",
      "6200 10255\n",
      "6300 10255\n",
      "6400 10255\n",
      "6500 10255\n",
      "6600 10255\n",
      "6700 10255\n",
      "6800 10255\n",
      "6900 10255\n",
      "7000 10255\n",
      "7100 10255\n",
      "7200 10255\n",
      "7300 10255\n",
      "7400 10255\n",
      "7500 10255\n",
      "7600 10255\n",
      "7700 10255\n",
      "7800 10255\n",
      "7900 10255\n",
      "8000 10255\n",
      "8100 10255\n",
      "8200 10255\n",
      "8300 10255\n",
      "8400 10255\n",
      "8500 10255\n",
      "8600 10255\n",
      "8700 10255\n",
      "8800 10255\n",
      "8900 10255\n",
      "9000 10255\n",
      "9100 10255\n",
      "9200 10255\n",
      "9300 10255\n",
      "9400 10255\n",
      "9500 10255\n",
      "9600 10255\n",
      "9700 10255\n",
      "9800 10255\n",
      "9900 10255\n",
      "10000 10255\n",
      "10100 10255\n",
      "10200 10255\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk.data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review2wordlist(review):\n",
    "    #junk words\n",
    "    junk=['http','www','com']\n",
    "    #Remove non-letters\n",
    "    review_text=re.sub(\"[^a-zA-Z]\",\" \",review)\n",
    "    #Convert words to lowercase and form a wordlist\n",
    "    review_text=review_text.lower().split()\n",
    "    #remove stop words\n",
    "    stops=set(stopwords.words(\"english\"))\n",
    "    review_text=[w for w in review_text if w not in stops]\n",
    "    review_text=[w for w in review_text if w not in junk]\n",
    "    #stem words \n",
    "    wnl=WordNetLemmatizer()\n",
    "    review_text=[wnl.lemmatize(w) for w in review_text]\n",
    "    \n",
    "    return review_text\n",
    "\n",
    "#For each review split it into sentence\n",
    "def review_to_sentences(review,tokenizer):\n",
    "    sentences=[]\n",
    "    texts=tokenizer.tokenize(review.strip())\n",
    "    sentences=[0]*len(texts)\n",
    "    i=0\n",
    "    for t in texts:\n",
    "        if len(t)>0:\n",
    "            #for each sentence split it into words\n",
    "            sentence=review2wordlist(t)\n",
    "            sentences[i]=sentence\n",
    "            i+=1\n",
    "    #return list of list\n",
    "    return sentences\n",
    "\n",
    "#transform paragraphs to list of lists\n",
    "for count in range(len(item_review)):\n",
    "    if count%100==0:\n",
    "        print(count,len(item_review))\n",
    "    individual_text=review_to_sentences(item_review[count],tokenizer)\n",
    "    item_review[count]=individual_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 22996\n",
      "1000 22996\n",
      "2000 22996\n",
      "3000 22996\n",
      "4000 22996\n",
      "5000 22996\n",
      "6000 22996\n",
      "7000 22996\n",
      "8000 22996\n",
      "9000 22996\n",
      "10000 22996\n",
      "11000 22996\n",
      "12000 22996\n",
      "13000 22996\n",
      "14000 22996\n",
      "15000 22996\n",
      "16000 22996\n",
      "17000 22996\n",
      "18000 22996\n",
      "19000 22996\n",
      "20000 22996\n",
      "21000 22996\n",
      "22000 22996\n"
     ]
    }
   ],
   "source": [
    "# list of list for user reviews\n",
    "for count in range(len(user_review)):\n",
    "    if count%1000==0:\n",
    "        print(count,len(user_review))\n",
    "    individual_text=review_to_sentences(user_review[count],tokenizer)\n",
    "    user_review[count]=individual_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### implement algorithm for word2vec on item reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#total reviews to get run by word2vec\n",
    "from functools import reduce\n",
    "total_reviews = reduce(lambda x,y: x+y,item_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "num_features = 300\n",
    "min_word_count=40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "#train the word2vec model\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model=word2vec.Word2Vec(total_reviews, workers=num_workers,size=num_features,min_count=min_word_count,window=context,sample=downsampling)\n",
    "model_name=\"amazonwordvec\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vec2pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/virtual/lib/python3.5/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "//anaconda/envs/virtual/lib/python3.5/site-packages/sklearn/preprocessing/data.py:184: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "words_array=model.syn0\n",
    "s = scale(words_array)\n",
    "pca=PCA(n_components=200)\n",
    "pca_model=pca.fit_transform(s)\n",
    "df = pd.DataFrame(pca_model)\n",
    "df[\"words\"] = model.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Dictionary with PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_dic={}\n",
    "features=df.ix[:,0:200]\n",
    "words=df[\"words\"]\n",
    "for i in range(len(features)):\n",
    "    pca_dic[words[i]]=list(np.transpose(features)[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform words in each review into vectors of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Length of each word is now length of PCA\n",
    "\n",
    "def makeFeatureVec(words, dic, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    pca_wordset = set(dic)\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in pca_wordset: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,dic[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return list(featureVec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10255\n",
      "1000 10255\n",
      "2000 10255\n",
      "3000 10255\n",
      "4000 10255\n",
      "5000 10255\n",
      "6000 10255\n",
      "7000 10255\n",
      "8000 10255\n",
      "9000 10255\n",
      "10000 10255\n"
     ]
    }
   ],
   "source": [
    "#Transform word list of each paragraph into average feature vectors\n",
    "item_vector=item_review\n",
    "for i in range(len(item_vector)):\n",
    "    if i%1000==0:\n",
    "        print(i,len(item_vector))\n",
    "    item_vector[i]=makeFeatureVec(sum(item_vector[i],[]),pca_dic,200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 22996\n",
      "1000 22996\n",
      "2000 22996\n",
      "3000 22996\n",
      "4000 22996\n",
      "5000 22996\n",
      "6000 22996\n",
      "7000 22996\n",
      "8000 22996\n",
      "9000 22996\n",
      "10000 22996\n",
      "11000 22996\n",
      "12000 22996\n",
      "13000 22996\n",
      "14000 22996\n",
      "15000 22996\n",
      "16000 22996\n",
      "17000 22996\n",
      "18000 22996\n",
      "19000 22996\n",
      "20000 22996\n",
      "21000 22996\n",
      "22000 22996\n"
     ]
    }
   ],
   "source": [
    "#Do the same transformation for users\n",
    "for i in range(len(user_review)):\n",
    "    if i%1000==0:\n",
    "        print(i,len(user_review))\n",
    "    user_review[i]=makeFeatureVec(sum(user_review[i],[]),pca_dic,200) \n",
    "    \n",
    "user_vector=user_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do cosine similarity on the item and user vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import heapq\n",
    "\n",
    "item=[]\n",
    "i=user_review[3]\n",
    "for j in item_review:\n",
    "        result = 1 - spatial.distance.cosine(i, j)\n",
    "        item.append(result)\n",
    "tops = heapq.nlargest(15, range(len(item)), item.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df_total[['ItemID','reviewText','Ratings']]\n",
    "x = x[x['Ratings'] == 5]\n",
    "grouped_x = x.groupby('ItemID')\n",
    "x=grouped_x['reviewText'].apply(\" \".join)\n",
    "\n",
    "y = df_total[['reviewerID','reviewText','Ratings']]\n",
    "y = y[y['Ratings'] == 5]\n",
    "grouped_y = y.groupby('reviewerID')\n",
    "y=grouped_y['reviewText'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a' in \"abc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [virtual]",
   "language": "python",
   "name": "Python [virtual]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
