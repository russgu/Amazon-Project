{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Game Product Sentiment Analysis using Vec2PCA\n",
    "\n",
    "## Intro\n",
    "\n",
    "Recommendation systems have been employed by various social network platforms in order to deliver a better service to their users. Based on user data, recommendation systems can infer user preference and make predictions to what products a user will be interested in and purchase. This work focuses on the review text of Amazon game products and applies Word2Vec technique and Principle Component Analysis to generate topic features from textual data.\n",
    "\n",
    "## Problem Definition and Algorithm\n",
    "### - Task: \n",
    "The task is to learn a vector space representation of word, and generate topics from the words using a dimensionality reduction technique called principle component analysis(PCA). In this way we can see groupings of particular words that resemble certain topics about the items. The aim is to classify the ratings as 1 or 0 using the PCA feature and obtain comparable accuracy to using a bad of words representation.\n",
    "\n",
    "### - Algorithm:\n",
    "#### Word2Vec:\n",
    "Word2Vec is a deep learning algorithm that maps individual words into a vector in a low dimensional space. Let d be the number of dimensions, and N be the number of words, then one learns vectors v_word for each word, an N x d matrix W, and a d dimension vector of bias terms b, with the property that the average multinomial log loss (\"cross-entropy\") of \"nearby_word ~ v_word*W + b\" is minimized. \n",
    "There are 2*N*d total parameters to learn, N*d for W and d for each v_word for each of the N words \n",
    "W is fixed, uniform over v_word\n",
    "\n",
    "\n",
    "\n",
    "#### PCA:\n",
    "PCA a dimensionality reduction technique where the first principle component is the direction which captures data with the highest variance, and the second principle component is perpendicular to the first one while captures as much as variance in the data as possible, and so on and so forth.The number of principle components can be as many as the number of variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_text=[0]*231780\n",
    "ratings=[0]*231780\n",
    "itemID=[0]*231780\n",
    "reviewerID=[0]*231780\n",
    "\n",
    "i=0\n",
    "with open(\"reviews_Video_Games_5.json\",\"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        data=json.loads(line) \n",
    "        review_text[i]=data[\"reviewText\"]\n",
    "        reviewerID[i]=data[\"reviewerID\"]\n",
    "        itemID[i]=data[\"asin\"]\n",
    "        ratings[i]=data[\"overall\"]\n",
    "        i+=1\n",
    "\n",
    "reviewerID = pd.DataFrame({'reviewerID': reviewerID})\n",
    "ratings = pd.DataFrame({'Ratings': ratings})\n",
    "itemID = pd.DataFrame({'ItemID':itemID})\n",
    "review_text = pd.DataFrame({'reviewText':review_text})\n",
    "\n",
    "df_predict = pd.concat([ratings,reviewerID,itemID,review_text], axis=1)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk.data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review2wordlist(review):\n",
    "    #Remove non-letters\n",
    "    review_text=re.sub(\"[^a-zA-Z]\",\" \",review)\n",
    "    #Convert words to lowercase and form a wordlist\n",
    "    review_text=review_text.lower().split()\n",
    "    #remove stop words\n",
    "    stops=set(stopwords.words(\"english\"))\n",
    "    review_text=[w for w in review_text if w not in stops]\n",
    "    #stem words \n",
    "    wnl=WordNetLemmatizer()\n",
    "    review_text=[wnl.lemmatize(w) for w in review_text]\n",
    "    \n",
    "    return review_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 231780\n",
      "1000 231780\n",
      "2000 231780\n",
      "3000 231780\n",
      "4000 231780\n",
      "5000 231780\n",
      "6000 231780\n",
      "7000 231780\n",
      "8000 231780\n",
      "9000 231780\n",
      "10000 231780\n",
      "11000 231780\n",
      "12000 231780\n",
      "13000 231780\n",
      "14000 231780\n",
      "15000 231780\n",
      "16000 231780\n",
      "17000 231780\n",
      "18000 231780\n",
      "19000 231780\n",
      "20000 231780\n",
      "21000 231780\n",
      "22000 231780\n",
      "23000 231780\n",
      "24000 231780\n",
      "25000 231780\n",
      "26000 231780\n",
      "27000 231780\n",
      "28000 231780\n",
      "29000 231780\n",
      "30000 231780\n",
      "31000 231780\n",
      "32000 231780\n",
      "33000 231780\n",
      "34000 231780\n",
      "35000 231780\n",
      "36000 231780\n",
      "37000 231780\n",
      "38000 231780\n",
      "39000 231780\n",
      "40000 231780\n",
      "41000 231780\n",
      "42000 231780\n",
      "43000 231780\n",
      "44000 231780\n",
      "45000 231780\n",
      "46000 231780\n",
      "47000 231780\n",
      "48000 231780\n",
      "49000 231780\n",
      "50000 231780\n",
      "51000 231780\n",
      "52000 231780\n",
      "53000 231780\n",
      "54000 231780\n",
      "55000 231780\n",
      "56000 231780\n",
      "57000 231780\n",
      "58000 231780\n",
      "59000 231780\n",
      "60000 231780\n",
      "61000 231780\n",
      "62000 231780\n",
      "63000 231780\n",
      "64000 231780\n",
      "65000 231780\n",
      "66000 231780\n",
      "67000 231780\n",
      "68000 231780\n",
      "69000 231780\n",
      "70000 231780\n",
      "71000 231780\n",
      "72000 231780\n",
      "73000 231780\n",
      "74000 231780\n",
      "75000 231780\n",
      "76000 231780\n",
      "77000 231780\n",
      "78000 231780\n",
      "79000 231780\n",
      "80000 231780\n",
      "81000 231780\n",
      "82000 231780\n",
      "83000 231780\n",
      "84000 231780\n",
      "85000 231780\n",
      "86000 231780\n",
      "87000 231780\n",
      "88000 231780\n",
      "89000 231780\n",
      "90000 231780\n",
      "91000 231780\n",
      "92000 231780\n",
      "93000 231780\n",
      "94000 231780\n",
      "95000 231780\n",
      "96000 231780\n",
      "97000 231780\n",
      "98000 231780\n",
      "99000 231780\n",
      "100000 231780\n",
      "101000 231780\n",
      "102000 231780\n",
      "103000 231780\n",
      "104000 231780\n",
      "105000 231780\n",
      "106000 231780\n",
      "107000 231780\n",
      "108000 231780\n",
      "109000 231780\n",
      "110000 231780\n",
      "111000 231780\n",
      "112000 231780\n",
      "113000 231780\n",
      "114000 231780\n",
      "115000 231780\n",
      "116000 231780\n",
      "117000 231780\n",
      "118000 231780\n",
      "119000 231780\n",
      "120000 231780\n",
      "121000 231780\n",
      "122000 231780\n",
      "123000 231780\n",
      "124000 231780\n",
      "125000 231780\n",
      "126000 231780\n",
      "127000 231780\n",
      "128000 231780\n",
      "129000 231780\n",
      "130000 231780\n",
      "131000 231780\n",
      "132000 231780\n",
      "133000 231780\n",
      "134000 231780\n",
      "135000 231780\n",
      "136000 231780\n",
      "137000 231780\n",
      "138000 231780\n",
      "139000 231780\n",
      "140000 231780\n",
      "141000 231780\n",
      "142000 231780\n",
      "143000 231780\n",
      "144000 231780\n",
      "145000 231780\n",
      "146000 231780\n",
      "147000 231780\n",
      "148000 231780\n",
      "149000 231780\n",
      "150000 231780\n",
      "151000 231780\n",
      "152000 231780\n",
      "153000 231780\n",
      "154000 231780\n",
      "155000 231780\n",
      "156000 231780\n",
      "157000 231780\n",
      "158000 231780\n",
      "159000 231780\n",
      "160000 231780\n",
      "161000 231780\n",
      "162000 231780\n",
      "163000 231780\n",
      "164000 231780\n",
      "165000 231780\n",
      "166000 231780\n",
      "167000 231780\n",
      "168000 231780\n",
      "169000 231780\n",
      "170000 231780\n",
      "171000 231780\n",
      "172000 231780\n",
      "173000 231780\n",
      "174000 231780\n",
      "175000 231780\n",
      "176000 231780\n",
      "177000 231780\n",
      "178000 231780\n",
      "179000 231780\n",
      "180000 231780\n",
      "181000 231780\n",
      "182000 231780\n",
      "183000 231780\n",
      "184000 231780\n",
      "185000 231780\n",
      "186000 231780\n",
      "187000 231780\n",
      "188000 231780\n",
      "189000 231780\n",
      "190000 231780\n",
      "191000 231780\n",
      "192000 231780\n",
      "193000 231780\n",
      "194000 231780\n",
      "195000 231780\n",
      "196000 231780\n",
      "197000 231780\n",
      "198000 231780\n",
      "199000 231780\n",
      "200000 231780\n",
      "201000 231780\n",
      "202000 231780\n",
      "203000 231780\n",
      "204000 231780\n",
      "205000 231780\n",
      "206000 231780\n",
      "207000 231780\n",
      "208000 231780\n",
      "209000 231780\n",
      "210000 231780\n",
      "211000 231780\n",
      "212000 231780\n",
      "213000 231780\n",
      "214000 231780\n",
      "215000 231780\n",
      "216000 231780\n",
      "217000 231780\n",
      "218000 231780\n",
      "219000 231780\n",
      "220000 231780\n",
      "221000 231780\n",
      "222000 231780\n",
      "223000 231780\n",
      "224000 231780\n",
      "225000 231780\n",
      "226000 231780\n",
      "227000 231780\n",
      "228000 231780\n",
      "229000 231780\n",
      "230000 231780\n",
      "231000 231780\n"
     ]
    }
   ],
   "source": [
    "#Transform Paragraphs to list of lists\n",
    "total=df_predict['reviewText']\n",
    "review_list=[0]*231780\n",
    "for i in range(len(total)):\n",
    "    if i%1000==0:\n",
    "        print(i,len(total))\n",
    "    x=review2wordlist(total[i])\n",
    "    review_list[i]=x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform words in each review into vectors of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Length of each word is now length of PCA\n",
    "\n",
    "def makeFeatureVec(words, dic, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    pca_wordset = set(dic)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in pca_wordset: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,dic[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return list(featureVec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform word list of each paragraph into average feature vectors\n",
    "reviews_vec=[0]*231780\n",
    "for i in range(len(review_list)):\n",
    "    if i%10000==0:\n",
    "        print(i,len(review_list))\n",
    "    reviews_vec[i]=makeFeatureVec(review_list[i],pca_dic,200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Elastic Net Regression against the PCA feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature=np.array(reviews_vec)\n",
    "target=np.array(df_predict['Ratings'])\n",
    "\n",
    "#transform target into 1 and 0\n",
    "for i in range(len(target)):\n",
    "    if target[i]>3:\n",
    "        target[i]=1\n",
    "    else:\n",
    "        target[i]=0\n",
    "        \n",
    "#throw out NAs in feature and target\n",
    "nas=np.argwhere(np.isnan(feature))\n",
    "NAs=[]\n",
    "for i in range(len(nas)):\n",
    "    if nas[i][1]%200==0:\n",
    "        NAs.append(nas[i][0])\n",
    "        \n",
    "target=np.delete(target,NAs)\n",
    "feature=np.delete(feature,NAs, 0)\n",
    "\n",
    "elastic_net=pd.DataFrame(feature)\n",
    "elastic_net['response']=target\n",
    "elastic_net.to_csv('/Users/guzi/Desktop/SignalDataScience/final project/elastic_net.csv')\n",
    "\n",
    "##RUN ELASTIC_NET IN R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R script for Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "Attaching package: ‘pROC’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "Loading required package: Matrix\n",
      "Loading required package: foreach\n",
      "Loaded glmnet 2.0-5\n",
      "\n",
      "\n",
      "Attaching package: ‘glmnet’\n",
      "\n",
      "The following object is masked from ‘package:pROC’:\n",
      "\n",
      "    auc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "setwd('/Users/guzi/Desktop/SignalDataScience/final project')\n",
    "\n",
    "library(dplyr)\n",
    "library(caret)\n",
    "library(readr)\n",
    "library(pROC)\n",
    "library(glmnet)\n",
    "\n",
    "df=read.csv(\"elastic_net.csv\")\n",
    "df=df[-1]\n",
    "response.pca=df$response\n",
    "features.pca= scale(df[1:200])\n",
    "\n",
    "floor(nrow(features.pca)*0.9)\n",
    "\n",
    "m = glmnet(features.pca[1:185380,], response.pca[1:185380], family = \"binomial\", alpha = 0, lambda = 0)\n",
    "prob.pca= predict(m, s='lambda.min', newx=features.pca[185380:231725,], type=\"response\")\n",
    "r=roc(response.pca[185380:231725],prob.pca)\n",
    "plot(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Bag of Words Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transform review_list into a list of sentences\n",
    "for i in range(len(review_list)):\n",
    "    review_list[i]=\" \".join(review_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#review_list is the bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "for i in NAs:\n",
    "    del review_list[i]\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",tokenizer = None,preprocessor = None,stop_words = None,max_features = 5000)\n",
    "features = vectorizer.fit_transform(review_list)\n",
    "features = features.toarray()\n",
    "\n",
    "target=np.array(df_predict['Ratings'])\n",
    "for i in range(len(target)):\n",
    "    if target[i]>3:\n",
    "        target[i]=1\n",
    "    else:\n",
    "        target[i]=0\n",
    "target=np.delete(target,NAs)\n",
    "        \n",
    "bagging=pd.DataFrame(features)\n",
    "bagging['response']=target\n",
    "\n",
    "feats=csr_matrix(features)\n",
    "#Put response into a dataframe\n",
    "pd.DataFrame(target).to_csv('/Users/guzi/Desktop/SignalDataScience/final project/bagging_target.csv')\n",
    "#Put feature into a matrix\n",
    "scipy.io.mmwrite(\"bagging_feature\", feats, comment='', field=None, precision=None, symmetry=None)\n",
    "\n",
    "##RUN ELASTIC_NET IN R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Find out the number of popular items that are in the test set\n",
    "# count=0\n",
    "# for i in range(len(test_all)):\n",
    "#     for ele in test_all[i]:\n",
    "#         if ele in popular_items:\n",
    "#             count+=1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Counting the number of recommendations that fall into the test set\n",
    "# count=0\n",
    "# for i in range(len(recommends_all)):\n",
    "#     for ele in recommends_all[i]:\n",
    "#         if ele in test_all[i]:\n",
    "#             count+=1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #find out the top 100 most popular items\n",
    "# popularity=[len(item_review[i]) for i in range(len(item_review))]\n",
    "# tops = heapq.nlargest(100, range(len(popularity)), popularity.__getitem__)\n",
    "# popular_items=[item_review.index[i] for i in tops]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
